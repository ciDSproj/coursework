{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Topic Identification using RNN and LSTM\n",
    "## Author - Catalina Ifrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UW DATASCI420-Machine Learning Techniques\n",
    "L10-LSTM_Text_Analysis\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "For this assignment, you will leverage the RNN_KERAS.ipynb lab in the lesson. You are tasked to use the Keras Reuters newswire\n",
    "topics classification dataset to **build a model that classifies the topic of each article or newswire**. \n",
    "Using the Keras dataset, create a new notebook and perform each of the following data preparation tasks and answer the related\n",
    "questions:\n",
    "\n",
    "1. Read Reuters dataset into training and testing \n",
    "2. Prepare dataset\n",
    "3. **Build and compile 3 different models using Keras **LSTM (Long Short-Term Memory)** ideally improving model at each iteration.\n",
    "4. Describe and explain your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset description\n",
    "\n",
    "The Keras Reuters newswire topics dataset contains 11,228 newswires from Reuters, labeled with over 46 topics. Each wire is\n",
    "encoded as a sequence of word indexes. For convenience, words are indexed by overall frequency in the dataset, so that for \n",
    "instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such \n",
    "as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\". As a convention, \"0\" does not\n",
    "stand for a specific word, but instead is used to encode any unknown word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Reuters dataset into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'reuters' dataset is loaded from keras, split into training and testing sets, then printed the shape of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.reuters\n",
    "\n",
    "# Load the dataset and split it into training and testing\n",
    "num_of_words=10000\n",
    "(X_train, y_train), (X_test, y_test) = data.load_data(num_words=num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (8982,)\n",
      "y_train shape (8982,)\n",
      "X_test shape (2246,)\n",
      "y_test shape (2246,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we check the data by printing the first record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the input consists of numbers instead of words. Corresponding to dataset description, each wire is \n",
    "encoded as a sequence of word indexes, where the words are indexed by overall frequency in the dataset.\n",
    "\n",
    "Below it is created a dictionary that maps words to integer index, Then it is defined a function that decodes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = tf.keras.datasets.reuters.get_word_index()\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2           # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Define function to decode data\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the decode function above it is decoded the first newswire and printed the actual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<START> <UNK> <UNK> said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the training data using the function above\n",
    "decode_review(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequences need to be modified so they all have the same length for modeling. For this, it is used the preprocessing \n",
    "library within keras. It is defined the max_review_length which is the maxim number of words in a newswire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the max number of words in a newswire and modify the input sequences\n",
    "max_review_length = 400 \n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output column is one-hot encoded using keras numpy-related utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (8982,)\n",
      "Shape after one-hot encoding:  (8982, 46)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding using keras' numpy-related utilities\n",
    "n_classes = 46\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = utils.to_categorical(y_train, n_classes)\n",
    "Y_test = utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build and compile 3 different models using Keras LSTM ideally improving model at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model has sequential layers with an input layer, an **LSTM (Long Short-Term Memory)** layer, and a dense output \n",
    "layer. \n",
    "- The input layer is an Embedding layer which takes 3 arguments:\n",
    "    - num-of-words = 10000 - the maximum number of words to be used (the most frequent words)\n",
    "    - embeding-vector-lenght = 32 - uses 32 length vectors to represent each word\n",
    "    - input_lenght = max_review_length (400) - the maximum number of words in a newswire\n",
    "- The second layer is an LTSM layer with 100 memory units. \n",
    "- The dense output layer must create 46 output values, one for each class (there are 46 topics). <br>\n",
    "\n",
    "The activation function is 'softmax' for multi-class classification. Since it is a multi-class classification problem, it is\n",
    "used 'categorical_crossentropy' as the loss function. The model is trained for 3 epochs and the batch_size value is 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                4646      \n",
      "=================================================================\n",
      "Total params: 377,846\n",
      "Trainable params: 377,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "141/141 [==============================] - 77s 541ms/step - loss: 2.8314 - accuracy: 0.3374 - val_loss: 2.4025 - val_accuracy: 0.3620\n",
      "Epoch 2/3\n",
      "141/141 [==============================] - 84s 593ms/step - loss: 2.1775 - accuracy: 0.4434 - val_loss: 1.9016 - val_accuracy: 0.5134\n",
      "Epoch 3/3\n",
      "141/141 [==============================] - 90s 641ms/step - loss: 1.9224 - accuracy: 0.5008 - val_loss: 2.0859 - val_accuracy: 0.5018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fa40226ac8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Model 1\n",
    "\n",
    "# num_of_words = 10000\n",
    "# max_review_length = 400\n",
    "\n",
    "embedding_vector_length = 32\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(keras.layers.LSTM(100))\n",
    "model.add(keras.layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.18%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **first model has an accuracy score of only 50%**. In the next model we'll try to improve this score by adding some\n",
    "more parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second model are added 'dropout' and 'recurrent_dropout' arguments for the LSTM layer. They are used to apply dropout \n",
    "probability to kernel and recurrent_kernel respectively. Both parameters are a float between 0 and 1.\n",
    "For this model it was selected a value of 0.2 for both 'dropout' and 'recurrent_dropout'. The model is trained for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 400, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 46)                4646      \n",
      "=================================================================\n",
      "Total params: 377,846\n",
      "Trainable params: 377,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "141/141 [==============================] - 135s 944ms/step - loss: 2.8355 - accuracy: 0.3289 - val_loss: 2.3950 - val_accuracy: 0.3620\n",
      "Epoch 2/5\n",
      "141/141 [==============================] - 191s 1s/step - loss: 2.1904 - accuracy: 0.4327 - val_loss: 1.8530 - val_accuracy: 0.5374\n",
      "Epoch 3/5\n",
      "141/141 [==============================] - 192s 1s/step - loss: 1.7571 - accuracy: 0.5413 - val_loss: 1.7509 - val_accuracy: 0.5534\n",
      "Epoch 4/5\n",
      "141/141 [==============================] - 205s 1s/step - loss: 1.6840 - accuracy: 0.5578 - val_loss: 1.7351 - val_accuracy: 0.5606\n",
      "Epoch 5/5\n",
      "141/141 [==============================] - 205s 1s/step - loss: 1.6669 - accuracy: 0.5753 - val_loss: 1.7266 - val_accuracy: 0.5779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fa4087af88>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Model 2\n",
    "\n",
    "# num_of_words = 10000\n",
    "# max_review_length = 400\n",
    "# embedding_vector_length = 32\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(keras.layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.79%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **second model achieved a better accuracy rate of 58%** comparing with the first model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third LSTM model was build using different values for the following parameters:\n",
    "- num-of-words=50000 - the maximum number of words to use was increased from 10000 to 50000\n",
    "- embeding-vector-lenght = 100 - instead of 32 lenght vectors, are used 100 length vectors to represent each word \n",
    "- max_review_length = 250 - the maximum number of words in a newswire was decreased from 400 to 250 words\n",
    "\n",
    "All the other hyperparameters remained the same as for Model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.reuters\n",
    "# Load the dataset and split it into training and testing\n",
    "num_of_words=50000\n",
    "(X_train, y_train), (X_test, y_test) = data.load_data(num_words=num_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before one-hot encoding:  (8982,)\n",
      "Shape after one-hot encoding:  (8982, 46)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding using keras' numpy-related utilities\n",
    "n_classes = 46\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train = utils.to_categorical(y_train, n_classes)\n",
    "Y_test = utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max number of words in a wire\n",
    "max_review_length = 250 \n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                4646      \n",
      "=================================================================\n",
      "Total params: 5,085,046\n",
      "Trainable params: 5,085,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "141/141 [==============================] - 95s 662ms/step - loss: 2.8257 - accuracy: 0.3322 - val_loss: 2.1005 - val_accuracy: 0.4795\n",
      "Epoch 2/5\n",
      "141/141 [==============================] - 117s 832ms/step - loss: 1.9803 - accuracy: 0.5000 - val_loss: 1.7433 - val_accuracy: 0.5548\n",
      "Epoch 3/5\n",
      "141/141 [==============================] - 116s 821ms/step - loss: 1.6794 - accuracy: 0.5537 - val_loss: 1.6967 - val_accuracy: 0.5614\n",
      "Epoch 4/5\n",
      "141/141 [==============================] - 117s 828ms/step - loss: 1.4920 - accuracy: 0.6086 - val_loss: 1.7260 - val_accuracy: 0.5467\n",
      "Epoch 5/5\n",
      "141/141 [==============================] - 122s 862ms/step - loss: 1.3052 - accuracy: 0.6544 - val_loss: 1.5083 - val_accuracy: 0.6287\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fc6fba5d48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Model 3\n",
    "\n",
    "# num_of_words = 50000\n",
    "# max_review_length = 250\n",
    "embedding_vector_length = 100\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(num_of_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(keras.layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(keras.layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.87%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **third model achieved an accuracy rate of 63%**.  This is the best accuracy score from all three models.\n",
    "\n",
    "There were trained and tested also a couple more models, but their accuracy rate was lower than for the third model. <br>\n",
    "One of the models was built using a 'SpatialDropout1D' layer (which performs variational dropout in NLP models); it had an \n",
    "accuracy rate of 57%. Other model used 128 memory units for the LSTM layer instead of 100 and different values for \n",
    "dropout and recurrent_dropout parameters; it achieved an accuracy rate of 55%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Describe and explain your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first LSTM model built achieved an accuracy score of 50%. \n",
    "\n",
    "For the second model, adding the 'dropout' and 'recurrent_dropout' regularization parameters to the LSTM layer improved the first model's accuracy rate to 58%.\n",
    "\n",
    "**The best accuracy score of 63% was obtained for the third LSTM model**. Comparing with the other two models, the **third model** had \n",
    "a **higher value for the maximum number of words**, a **higher length vectors to represent each word**, and a **smaller maximum \n",
    "number of words in a newswire**. \n",
    "\n",
    "All three models had the same number of memory units for the LSTM layer. \n",
    "\n",
    "There were also trained and tested a couple of other models. One of these models used a **'SpatialDropout1D' layer**. Another \n",
    "model had a higher number of memory units for the LSTM layer and used various values for dropout and recurrent_dropout \n",
    "parameters. These hyperparameter modifications did not improve though too much their accuracy score, which stayed at the same\n",
    "level as for the second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
